{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Introdução\n",
    "Este nottebook é destinado a análises de textos de reclamações de empresas de ecommerce. Para isso, são carregados os dados de abreviatura dos estados do Brasil, dados sobre o numero de clientes de cada empresa analisada, os dados das reclamações extraidas do reclame aqui  e os aquivos necessários para o préprocessamento. \n",
    "\n",
    "Os dados são ajustados e manipulados e algumas estatististica basiscas são realizadas e mostrada através de gráficos. Por fim, os dados são preprocesados e são realizadas a modelagem de tópico para em todos os dados e nos dados divididos em regiões. \n",
    "\n",
    "![Reclame Aqui](https://www.reclameaqui.com.br/_nuxt/assets/imgs/reclame-aqui-logo.svg) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 1. Importação da bibliotecas necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ecommerce-consumidor-gov/submarino.csv\n",
      "/kaggle/input/ecommerce-consumidor-gov/americanas.csv\n",
      "/kaggle/input/ecommerce-consumidor-gov/magazineluiza.csv\n",
      "/kaggle/input/ecommerce-consumidor-gov/mercado_livre.csv\n",
      "/kaggle/input/ecommerce-consumidor-gov/netshoes.csv\n",
      "/kaggle/input/preprocessamento/stopwords_pt_nltk.txt\n",
      "/kaggle/input/preprocessamento/estados_brasil.csv\n",
      "/kaggle/input/preprocessamento/abreviacao.csv\n",
      "/kaggle/input/preprocessamento/regionalismos.csv\n",
      "/kaggle/input/preprocessamento/letras.csv\n",
      "/kaggle/input/preprocessamento/LIWC2007_Portugues_win.dic.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from time import time\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk \n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# 2. Carregamento dos dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2.1 Carregar nomes e abreviatura dos estados do Brasil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados com as abreviaturas foi carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "caminho_estados = \"/essenciais/\"\n",
    "# carregar estados do Brasil\n",
    "df_estados = pd.read_csv(caminho_estados+\"estados_brasil.csv\", sep=\",\",engine=\"python\", error_bad_lines=False)\n",
    "\n",
    "print(\"Dados com as abreviaturas foi carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2.2 Carregar número de clientes por empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# caminho_numero_clientes = \"/kaggle/input/preprocessamento/\"\n",
    "# # carregar dados do numero de clientes providos pela anatel \n",
    "# df_clientes_por_companhia = pd.read_csv(caminho_numero_clientes+\"acessos_corrigidos.csv\", sep=\";\",engine=\"python\", error_bad_lines=False)\n",
    "# # df_clientes_states=df_clientes_states[(df_clientes_states.tecnologia == 'GSM') | (df_clientes_states.tecnologia == 'LTE') \n",
    "# #                                       | (df_clientes_states.tecnologia ==  'WCDMA')]\n",
    "# print(\"Dados com o número de clientes foi carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2.3 Carregar dados das reclamações "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Carregar dados reclame aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1740: '\t' expected after '\"'\n",
      "Skipping line 2781: '\t' expected after '\"'\n",
      "Skipping line 3220: '\t' expected after '\"'\n",
      "Skipping line 3898: '\t' expected after '\"'\n",
      "Skipping line 12109: '\t' expected after '\"'\n",
      "Skipping line 12217: '\t' expected after '\"'\n",
      "Skipping line 12952: '\t' expected after '\"'\n",
      "Skipping line 13638: '\t' expected after '\"'\n",
      "Skipping line 13890: '\t' expected after '\"'\n",
      "Skipping line 14755: '\t' expected after '\"'\n",
      "Skipping line 2273: '\t' expected after '\"'\n",
      "Skipping line 2970: '\t' expected after '\"'\n",
      "Skipping line 5042: '\t' expected after '\"'\n",
      "Skipping line 5532: '\t' expected after '\"'\n",
      "Skipping line 5533: '\t' expected after '\"'\n",
      "Skipping line 9245: '\t' expected after '\"'\n",
      "Skipping line 21966: '\t' expected after '\"'\n",
      "Skipping line 22402: '\t' expected after '\"'\n",
      "Skipping line 25798: '\t' expected after '\"'\n",
      "Skipping line 944: '\t' expected after '\"'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dados do Reclame aqui das companhias de telecomunicações carregados com suscesso!\n",
      "\n",
      "Quantidade de reclamações\n",
      "americanas_com_loja_online - 16456 reclamações\n",
      "magazine-luiza-loja-online - 5694 reclamações\n",
      "mercado-livre - 26346 reclamações\n",
      "netshoes - 4377 reclamações\n",
      "submarino - 5169 reclamações\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1893: '\t' expected after '\"'\n",
      "Skipping line 3394: '\t' expected after '\"'\n"
     ]
    }
   ],
   "source": [
    "caminho_reclamacoes = \"../dados_coletados/consumidor_gov/\"\n",
    "# caminho_reclamacoes = \"../../datasets/reclame_aqui/\"\n",
    "df_americanas_com_loja_online = pd.read_csv(caminho_reclamacoes+\"americanas.csv\", sep=\"\\t\",engine=\"python\", error_bad_lines=False)\n",
    "df_magazine_luiza_loja_online = pd.read_csv(caminho_reclamacoes+\"magazineluiza.csv\", sep=\"\\t\",engine=\"python\", error_bad_lines=False)\n",
    "df_mercado_livre = pd.read_csv(caminho_reclamacoes+\"mercado_livre.csv\", sep=\"\\t\",engine=\"python\", error_bad_lines=False)\n",
    "df_netshoes = pd.read_csv(caminho_reclamacoes+\"netshoes.csv\", sep=\"\\t\",engine=\"python\", error_bad_lines=False)\n",
    "df_submarino = pd.read_csv(caminho_reclamacoes+\"submarino.csv\", sep=\"\\t\",engine=\"python\", error_bad_lines=False)\n",
    "\n",
    "print(\"\\n\\nDados do Reclame aqui das companhias de telecomunicações carregados com suscesso!\")\n",
    "\n",
    "print(\"\\nQuantidade de reclamações\")\n",
    "print(\"americanas_com_loja_online - \"+str(len(df_americanas_com_loja_online))+\" reclamações\")\n",
    "print(\"magazine-luiza-loja-online - \"+str(len(df_magazine_luiza_loja_online))+\" reclamações\")\n",
    "print(\"mercado-livre - \"+str(len(df_mercado_livre))+\" reclamações\")\n",
    "print(\"netshoes - \"+str(len(df_netshoes))+\" reclamações\")\n",
    "print(\"submarino - \"+str(len(df_submarino))+\" reclamações\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empresa</th>\n",
       "      <th>status</th>\n",
       "      <th>data</th>\n",
       "      <th>local</th>\n",
       "      <th>relato</th>\n",
       "      <th>tempo_resposta</th>\n",
       "      <th>resposta</th>\n",
       "      <th>nota</th>\n",
       "      <th>avaliacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Americanas.com</td>\n",
       "      <td>Resolvido</td>\n",
       "      <td>06/02/2019</td>\n",
       "      <td>Miracema do Tocantins - TO</td>\n",
       "      <td>Efetuei o pagamento do pedido XXXXX-X86685281,...</td>\n",
       "      <td>(10 dia(s) depois)</td>\n",
       "      <td>Olá Amanda Gomes Rocha!Devido ao prazo estipul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;não há comentários do consumidor&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Americanas.com</td>\n",
       "      <td>Resolvido</td>\n",
       "      <td>11/02/2019</td>\n",
       "      <td>Altônia - PR</td>\n",
       "      <td>Cliente informa que fez compra na Lojas Americ...</td>\n",
       "      <td>(10 dia(s) depois)</td>\n",
       "      <td>Olá Elizeu!Em atendimento à demanda registrada...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;não há comentários do consumidor&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          empresa     status         data                       local  \\\n",
       "0  Americanas.com  Resolvido   06/02/2019  Miracema do Tocantins - TO   \n",
       "1  Americanas.com  Resolvido   11/02/2019                Altônia - PR   \n",
       "\n",
       "                                              relato      tempo_resposta  \\\n",
       "0  Efetuei o pagamento do pedido XXXXX-X86685281,...  (10 dia(s) depois)   \n",
       "1  Cliente informa que fez compra na Lojas Americ...  (10 dia(s) depois)   \n",
       "\n",
       "                                            resposta  nota  \\\n",
       "0  Olá Amanda Gomes Rocha!Devido ao prazo estipul...   NaN   \n",
       "1  Olá Elizeu!Em atendimento à demanda registrada...   NaN   \n",
       "\n",
       "                            avaliacao  \n",
       "0  <não há comentários do consumidor>  \n",
       "1  <não há comentários do consumidor>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_americanas_com_loja_online.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2.4 Carregar dados para o Pré-processamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregamento dos dados para o pre-processamento finalizado! \n"
     ]
    }
   ],
   "source": [
    "# Carregamento de dados para processar abreviação\n",
    "dicionario_dados_abreviacao = {}\n",
    "with open('/essenciais/abreviacao.csv',  encoding=\"utf-8-sig\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        dicionario_dados_abreviacao[row[0]]= row[1]\n",
    "        \n",
    "        \n",
    "# carregando alfabeto\n",
    "lista_dados_letras_alfabeto = []\n",
    "with open('/essenciais/letras.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        lista_dados_letras_alfabeto.append(row[0])\n",
    "        \n",
    "# carregando stopwords\n",
    "stop_words = []\n",
    "with open('/essenciais/stopwords_pt_nltk.txt') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        stop_words.append(row[0])\n",
    "\n",
    "\n",
    " # carregando regionalismos\n",
    "lista_dados_regionalismos = []\n",
    "with open('/essenciais/regionalismos.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        lista_dados_regionalismos.append(row[0])\n",
    "        \n",
    "print(\"Carregamento dos dados para o pre-processamento finalizado! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# 3. Métodos \n",
    "Nesta subseção são definidos os metodos usados no decorrer do notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " ## 3.1 Metodo para a separação\n",
    " Este metodo pode ser usado para separa dados de uma coluna, de acordo com os parametros passados. \n",
    "Ele será usado para separar a coluna \"Created\" em data e hora, e a coluna \"city_state\" e cidade e estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo para a separação dos dados foi carregados com sucesso\n"
     ]
    }
   ],
   "source": [
    "def separa_coluna(data, spli):\n",
    "    first = []\n",
    "    second = []\n",
    "    for da in data:\n",
    "        temp = da.split(spli)\n",
    "        first.append(temp[0])\n",
    "        second.append(temp[1])\n",
    "    return first, second\n",
    "\n",
    "print(\"Metodo para a separação dos dados foi carregados com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 3.2 Métodos para po préprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métodos para o pré-processamento carregados!\n"
     ]
    }
   ],
   "source": [
    "# Remover abrevoações\n",
    "abbreviations_regex = r\"|\".join([r\"\\b\" + key + r\"\\b\" for key in dicionario_dados_abreviacao.keys()])\n",
    "def transformar_abreviacao_em_palavras( text):\n",
    "    matches = re.findall(abbreviations_regex, text)\n",
    "    for match in matches:\n",
    "        if match in dicionario_dados_abreviacao.keys():\n",
    "            match_regex = r\"\\b\" + match + r\"\\b\"\n",
    "            replacement = dicionario_dados_abreviacao[match]\n",
    "            text = re.sub(match_regex, replacement, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remover repetição de letras\n",
    "def remover_repeticao_de_letras( text):\n",
    "\n",
    "    for match in lista_dados_letras_alfabeto:\n",
    "        match_regex = match + r\"a{3,}|b{3,}|c{3,}|d{3,}|e{3,}|f{3,}|g{3,}|h{3,}|i{3,}|j{3,}|k{3,k}|l{3,}|m{3,}|n{3,}|o{3,}|p{3,}|q{3,}|r{3,}|s{3,}|t{3,}|u{3,}|v{3,}|w{3,}|x{3,}|y{3,}|z{3,}\"\n",
    "        replacement = match\n",
    "        text = re.sub(match_regex, replacement, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remover stopwords\n",
    "def remover_stopwords(texts):\n",
    "    a=[]\n",
    "    for word in word_tokenize(str(texts)):\n",
    "        if word not in stop_words:\n",
    "            a.append(word)\n",
    "    \n",
    "    return a \n",
    "\n",
    "# remover regionalismo\n",
    "def remover_regionalismo(text):\n",
    "    for aa in lista_dados_regionalismos:\n",
    "        text = re.sub(r\"\\b\"+aa+r\"\\b\", '', str(text))\n",
    "    return text\n",
    "\n",
    "# remover saudações \n",
    "def remover_saudacao(text):\n",
    "    for aa in [\"ola\",\"oi\",\"bom dia\",\"boa tarde\",\"boa noite\"]:\n",
    "        text = re.sub(r\"\\b\"+aa+r\"\\b\", '', str(text))\n",
    "\n",
    "    return text\n",
    "# remover acentuação\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# fazer tokenização\n",
    "def Tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence\n",
    "\n",
    "# stemming das palavras\n",
    "def Stemming(sentence):\n",
    "    sentence = Tokenize(sentence)\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return u\" \".join(word for word in phrase)\n",
    "\n",
    "# remover nome empresa \n",
    "def remover_empresa_nome(text):\n",
    "    empresas = [\n",
    "        \"mercadolivre\",\n",
    "        \"mercado-livre\",\n",
    "        \"mercado_livre\",\n",
    "        \"submarino\",\n",
    "        \"magazineluiza.com\",\n",
    "        \"magazineluiza\",\n",
    "        \"americanas.com\", \n",
    "        \"americanas\", \n",
    "        \"netshoes\",\n",
    "    ]\n",
    "    for aa in empresas:\n",
    "        text = re.sub(r\"\\b\"+aa+r\"\\b\", '', str(text))\n",
    "\n",
    "    return text\n",
    "\n",
    "# Metodo para executar o preporcessamento \n",
    "def processar_dados(data):\n",
    "    \n",
    "     # remove stopwords\n",
    "    data = [' '.join(remover_stopwords(sent)) for sent in data]\n",
    "    \n",
    "    # lowerCase\n",
    "    data = [sent.lower() for sent in data]\n",
    "    \n",
    "    #remove abreviacao\n",
    "    data = [transformar_abreviacao_em_palavras(sent) for sent in data]\n",
    "\n",
    "    #remove BRs\n",
    "    data = [re.sub(r\"<br \\/>\", ' ', sent) for sent in data]\n",
    "    \n",
    "     # Remove URLs\n",
    "    data = [re.sub(\"http://\\S+\\s*\", \"\", sent) for sent in data]\n",
    "    data = [re.sub(\"https://\\S+\\s*\", \"\", sent) for sent in data]\n",
    "    data = [re.sub(\"www.\\S+\\s*\", \"\", sent) for sent in data]\n",
    "    \n",
    "    # Acentuação \n",
    "    data = [ str(sent).encode('latin-1', 'ignore').decode('latin-1') for sent in data]\n",
    "\n",
    "    # Transformar palavras para minusculo \n",
    "    data = [sent.lower() for sent in data]\n",
    "\n",
    "    # remove tags\n",
    "    data = [re.sub('\\<script.*script>', '', str(sent)) for sent in data]\n",
    "\n",
    "    data = [re.sub('\\<a.*a>', '', str(sent)) for sent in data]\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "    # remove HashTag\n",
    "    data = [re.sub(r\"#\\w*\", \"\", sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    #Remove RTs\n",
    "    data = [re.sub(\"RT\", \"\", sent) for sent in data]\n",
    "\n",
    "    # Remove numbers \n",
    "    data = [re.sub(r\"[0-9]+\", \"\", sent) for sent in data]\n",
    "\n",
    "    # Remove ponctuation \n",
    "    data = [re.sub(r\"\\.|,|;|!|\\?|\\ - |:\", \" \", sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # remove acentuation\n",
    "    data =  [remove_accents(sent) for sent in data]\n",
    "\n",
    "    # removeNoise\n",
    "    data = [re.sub(r\"\\\\|\\.|(?<!\\w)\\w(?!\\w)|>|<|,|\\`|;|:|\\+|-|_|\\*|@|#|\\$|%|&|\\(|\\)|\\[|\\]|\\{|\\}|~|\\^|/|\\'|\\\"|°|º\", \" \", sent) for sent in data]\n",
    "\n",
    "    # remove saudação\n",
    "    data = [remover_saudacao(sent) for sent in data]\n",
    "    \n",
    "    # remove saudação\n",
    "    data = [remover_empresa_nome(sent) for sent in data]\n",
    "    \n",
    "    # remove regionalismos\n",
    "    data = [remover_regionalismo(sent) for sent in data]\n",
    "    \n",
    "    # remove tags\n",
    "    data = [re.sub('\\<script.*script>', '', str(sent)) for sent in data]\n",
    "    \n",
    "#     remover tag \n",
    "    data = [re.sub('\\<a.*a>', '', str(sent)) for sent in data]\n",
    "    \n",
    "    # remover repetições como \"oiiiiiii\"\n",
    "    data = [remover_repeticao_de_letras(sent) for sent in data]\n",
    "    \n",
    "#     data = [Stemming(sent) for sent in data]\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Métodos para o pré-processamento carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# 4. Ajuste nos dados carregados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "## 4.1 Separar coluna \"created\" \n",
    "Nos dados das reclamações é necessário dividir a coluna \"created\" em outros duas colunas, chamadas de data e hora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4.1.1 Executando a separação e criando as duas novas colunas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_americanas_com_loja_online.created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# date, time_created = separa_coluna(list(df_americanas_com_loja_online.created),\"T\")\n",
    "# df_americanas_com_loja_online['date'] = date\n",
    "# df_americanas_com_loja_online['time_created'] = time_created\n",
    "\n",
    "# date, time_created = separa_coluna(list(df_magazine_luiza_loja_online.created),\"T\")\n",
    "# df_magazine_luiza_loja_online['date'] = date\n",
    "# df_magazine_luiza_loja_online['time_created'] = time_created\n",
    "\n",
    "# date, time_created = separa_coluna(list(df_mercado_livre.created),\"T\")\n",
    "# df_mercado_livre['date'] = date\n",
    "# df_mercado_livre['time_created'] = time_created\n",
    "\n",
    "# date, time_created = separa_coluna(list(df_netshoes.created),\"T\")\n",
    "# df_netshoes['date'] = date\n",
    "# df_netshoes['time_created'] = time_created\n",
    "\n",
    "# date, time_created = separa_coluna(list(df_submarino.created),\"T\")\n",
    "# df_submarino['date'] = date\n",
    "# df_submarino['time_created'] = time_created\n",
    "\n",
    "# print(\"Criação das colunas para data e para a hora foi finalizada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "## Separar coluna \"city_state\" em cidades e estados (consumidor.gov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criação das colunas para a cidade e para o estado foi finalizada\n"
     ]
    }
   ],
   "source": [
    "cidade, estado = separa_coluna(list(df_americanas_com_loja_online.local),\" - \")\n",
    "df_americanas_com_loja_online['cidade'] = cidade\n",
    "df_americanas_com_loja_online['estado'] = estado\n",
    "\n",
    "cidade, estado = separa_coluna(list(df_magazine_luiza_loja_online.local),\" - \")\n",
    "df_magazine_luiza_loja_online['cidade'] = cidade\n",
    "df_magazine_luiza_loja_online['estado'] = estado\n",
    "\n",
    "cidade, estado = separa_coluna(list(df_mercado_livre.local),\" - \")\n",
    "df_mercado_livre['cidade'] = cidade\n",
    "df_mercado_livre['estado'] = estado\n",
    "\n",
    "cidade, estado = separa_coluna(list(df_netshoes.local),\" - \")\n",
    "df_netshoes['cidade'] = cidade\n",
    "df_netshoes['estado'] = estado\n",
    "\n",
    "cidade, estado = separa_coluna(list(df_submarino.local),\" - \")\n",
    "df_submarino['cidade'] = cidade\n",
    "df_submarino['estado'] = estado\n",
    "\n",
    "print(\"Criação das colunas para a cidade e para o estado foi finalizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 4.2 Remover linha nos dataframes das empresas com dados incorretos na coluna \"state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mercado_livre.userState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remoção dos ruidos doi finalizada!\n"
     ]
    }
   ],
   "source": [
    "ruidos = ['*','None','G','C', 'B','E', 'P','--','-', 'M', 'R', 'S', 'naoconsta']\n",
    "\n",
    "for ruido in ruidos:\n",
    "    df_americanas_com_loja_online = df_americanas_com_loja_online[df_americanas_com_loja_online.estado !=ruido]\n",
    "    df_magazine_luiza_loja_online = df_magazine_luiza_loja_online[df_magazine_luiza_loja_online.estado != ruido]\n",
    "    df_mercado_livre = df_mercado_livre[df_mercado_livre.estado != ruido]\n",
    "    df_netshoes = df_netshoes[df_netshoes.estado != ruido]\n",
    "    df_submarino = df_submarino[df_submarino.estado != ruido]\n",
    "print(\"Remoção dos ruidos doi finalizada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 01/02/2019'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mercado_livre.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# df_tim_antes = copy.copy(df_tim)\n",
    "# df_vivo_antes = copy.copy(df_vivo)\n",
    "# df_oi_antes = copy.copy(df_oi)\n",
    "# df_claro_antes = copy.copy(df_claro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 6. Préprocessamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_americanas_com_loja_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# df_americanas_com_loja_online.description = processar_dados(list(df_americanas_com_loja_online.description))\n",
    "# df_magazine_luiza_loja_online.description = processar_dados(list(df_magazine_luiza_loja_online.description))\n",
    "# df_mercado_livre.description = processar_dados(list(df_mercado_livre.description))\n",
    "# df_netshoes.description = processar_dados(list(df_netshoes.description))\n",
    "# df_submarino.description = processar_dados(list(df_submarino.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_americanas_com_loja_online.relato = processar_dados(list(df_americanas_com_loja_online.relato))\n",
    "df_magazine_luiza_loja_online.relato = processar_dados(list(df_magazine_luiza_loja_online.relato))\n",
    "df_mercado_livre.relato = processar_dados(list(df_mercado_livre.relato))\n",
    "df_netshoes.relato = processar_dados(list(df_netshoes.relato))\n",
    "df_submarino.relato = processar_dados(list(df_submarino.relato))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tim.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_tim.description[2])\n",
    "# print(df_tim_antes.description[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_americanas_com_loja_online.to_csv(\"americanas_pre_cgov.csv\", sep='\\t', index=False)\n",
    "df_magazine_luiza_loja_online.to_csv(\"magazine_luiza_pre_cgov.csv\", sep='\\t', index=False)\n",
    "df_mercado_livre.to_csv(\"mercado_livre_pre_cgov.csv\", sep='\\t', index=False)\n",
    "df_netshoes.to_csv(\"netshoes_pre_cgov.csv\", sep='\\t', index=False)\n",
    "df_submarino.to_csv(\"submarino_pre_cgov.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
